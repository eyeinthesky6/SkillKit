# Final Check

**Purpose:** Comprehensive final validation before marking feature complete

---

## Phase 1: Run All Quality Gates

**Execute complete quality validation:**

```bash
FEATURE_ID="$1"

echo "ðŸ” FINAL CHECK: Comprehensive validation for ${FEATURE_ID:-'project'}"

# 1. Linting
echo "1. ðŸ§¹ Running linter..."
if [ -f "package.json" ]; then
  npm run lint 2>&1 | tee /tmp/final-lint.log
  LINT_ERR=$(grep -cE "error|âœ–|failed" /tmp/final-lint.log || echo "0")
elif [ -f "pyproject.toml" ]; then
  python -m flake8 src/ 2>&1 | tee /tmp/final-lint.log
  LINT_ERR=$(grep -cE "error|âœ–|E[0-9]" /tmp/final-lint.log || echo "0")
else
  echo "No linting configured"
  LINT_ERR=0
fi

# 2. Type checking
echo "2. ðŸ” Running type checker..."
if [ -f "package.json" ]; then
  npm run type-check 2>&1 | tee /tmp/final-type.log 2>/dev/null || npx tsc --noEmit 2>&1 | tee /tmp/final-type.log
  TYPE_ERR=$(grep -c "error" /tmp/final-type.log || echo "0")
elif [ -f "pyproject.toml" ]; then
  python -m mypy src/ 2>&1 | tee /tmp/final-type.log
  TYPE_ERR=$(grep -c "error:" /tmp/final-type.log || echo "0")
else
  echo "No type checking configured"
  TYPE_ERR=0
fi

# 3. Build
echo "3. ðŸ”¨ Running build..."
if [ -f "package.json" ]; then
  npm run build 2>&1 | tee /tmp/final-build.log
  BUILD_FAIL=$?
elif [ -f "pyproject.toml" ]; then
  python -m build 2>&1 | tee /tmp/final-build.log
  BUILD_FAIL=$?
else
  echo "Build check skipped"
  BUILD_FAIL=0
fi

# 4. Tests
echo "4. ðŸ§ª Running tests..."
if [ -f "package.json" ]; then
  npm test -- --watchAll=false 2>&1 | tee /tmp/final-test.log
  TEST_FAIL=$?
elif [ -f "pyproject.toml" ]; then
  python -m pytest 2>&1 | tee /tmp/final-test.log
  TEST_FAIL=$?
else
  echo "Test check skipped"
  TEST_FAIL=0
fi

# 5. Additional checks (customize as needed)
echo "5. ðŸ” Running additional checks..."

# Code duplication check
if command -v jscpd >/dev/null 2>&1; then
  npx jscpd src/ --min-lines 10 2>&1 | tee /tmp/final-dup.log
  DUP_ISSUES=$(grep -c "duplication" /tmp/final-dup.log || echo "0")
else
  DUP_ISSUES=0
fi

# TODO/Mock check
TODO_COUNT=$(grep -r "TODO\|FIXME\|MOCK\|STUB" src/ --include="*.ts" --include="*.js" --include="*.py" 2>/dev/null | wc -l || echo "0")

echo ""
echo "ðŸ“Š FINAL CHECK RESULTS:"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "Lint errors: $LINT_ERR"
echo "Type errors: $TYPE_ERR"
echo "Build status: $([ $BUILD_FAIL -eq 0 ] && echo 'âœ… PASS' || echo 'âŒ FAIL')"
echo "Test status: $([ $TEST_FAIL -eq 0 ] && echo 'âœ… PASS' || echo 'âŒ FAIL')"
echo "Duplications: $DUP_ISSUES"
echo "TODOs/Mocks: $TODO_COUNT"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
```

---

## Phase 2: Evaluate Results

**Determine next action based on findings:**

```bash
# Calculate total issues
TOTAL_ISSUES=$((LINT_ERR + TYPE_ERR + DUP_ISSUES + TODO_COUNT))
BUILD_STATUS=$([ $BUILD_FAIL -eq 0 ] && echo "PASS" || echo "FAIL")
TEST_STATUS=$([ $TEST_FAIL -eq 0 ] && echo "PASS" || echo "FAIL")

echo ""
echo "ðŸŽ¯ EVALUATION:"

# Critical failures - cannot proceed
if [ "$BUILD_STATUS" = "FAIL" ]; then
  echo "âŒ CRITICAL: Build failing - fix immediately"
  echo "ðŸ”€ Route: Use FIX_BUGS workflow for build issues"
  exit 1
fi

if [ $TYPE_ERR -gt 50 ]; then
  echo "âŒ CRITICAL: Too many type errors ($TYPE_ERR)"
  echo "ðŸ”€ Route: Use FEATURE_FIX_STRATEGY for systematic fixing"
  exit 1
fi

# Quality issues - fix before marking complete
if [ $LINT_ERR -gt 20 ] || [ $TYPE_ERR -gt 10 ]; then
  echo "âš ï¸  QUALITY: High error count needs attention"
  echo "ðŸ”€ Route: Use fix-all workflow for systematic cleanup"
  exit 1
fi

if [ "$TEST_STATUS" = "FAIL" ]; then
  echo "âš ï¸  TESTS: Test failures detected"
  echo "ðŸ”€ Route: Use CREATE_TESTS or FIX_BUGS for test issues"
  exit 1
fi

if [ $TODO_COUNT -gt 10 ]; then
  echo "âš ï¸  TODOS: High TODO count ($TODO_COUNT)"
  echo "ðŸ”€ Route: Use todo-execution workflow"
  exit 1
fi

# Minor issues - can proceed but should note
if [ $TOTAL_ISSUES -gt 0 ]; then
  echo "âœ… MINOR: Some issues remain but acceptable"
  echo "ðŸ“ Note: $TOTAL_ISSUES minor issues to address later"
  echo "ðŸ”€ Route: Mark feature complete, track issues for next sprint"
  exit 0
fi

# All clear
echo "âœ… PERFECT: All checks passed!"
echo "ðŸŽ‰ Feature ready for production"
exit 0
```

---

## Phase 3: Generate Completion Report

**Document final validation results:**

```bash
# Create completion report
REPORT_FILE="docs/audit/final_check_$(date +%Y-%m-%d_%H-%M-%S).md"

cat > "$REPORT_FILE" << EOF
# Final Check Report

**Feature:** ${FEATURE_ID:-'Project'}
**Date:** $(date)
**Status:** $([ $? -eq 0 ] && echo 'âœ… PASSED' || echo 'âŒ FAILED')

## Validation Results

### Code Quality
- Lint errors: $LINT_ERR
- Type errors: $TYPE_ERR
- Code duplications: $DUP_ISSUES
- TODO/Mock count: $TODO_COUNT

### Build & Test
- Build status: $BUILD_STATUS
- Test status: $TEST_STATUS

### Action Taken
$(if [ $TOTAL_ISSUES -eq 0 ]; then
  echo "- âœ… Feature marked complete"
  echo "- ðŸŽ‰ Ready for production"
else
  echo "- âš ï¸ Issues identified, routed to appropriate workflow"
  echo "- ðŸ“‹ Issues tracked for resolution"
fi)

## Next Steps
$(if [ $TOTAL_ISSUES -eq 0 ]; then
  echo "1. Deploy to staging/production"
  echo "2. Monitor for issues"
  echo "3. Update documentation"
else
  echo "1. Address issues in routed workflow"
  echo "2. Re-run final check"
  echo "3. Complete when all issues resolved"
fi)
EOF

echo "ðŸ“„ Report saved: $REPORT_FILE"
```

---

## Success Criteria

- âœ… All validation checks completed
- âœ… Results properly evaluated
- âœ… Appropriate routing decision made
- âœ… Completion report generated
- âœ… Clear next steps documented

---

## Routing Logic

**âŒ Cannot proceed (exit 1):**
- Build failures (critical)
- >50 type errors (overwhelming)

**âš ï¸ Fix required (exit 1):**
- >20 lint errors
- >10 type errors
- Test failures
- >10 TODOs

**âœ… Can proceed (exit 0):**
- <20 total issues
- Build and tests pass
- Acceptable quality level

---

**Commands:**
- Quality gate runners (`npm run lint`, `npm run type-check`, etc.)
- Build tools (`npm run build`, `python -m build`)
- Test runners (`npm test`, `pytest`)
- Analysis tools (duplication checkers, TODO counters)

**When to use:** Before marking any feature or project complete, as final quality gate
